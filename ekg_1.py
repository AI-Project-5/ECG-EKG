# -*- coding: utf-8 -*-
"""EKG-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1guWcx1WpYumUqkju95OGjUoLe0PvJVOG
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
zip_ref = zipfile.ZipFile(r'/content/drive/MyDrive/Content /NEW DATASET EKG.xlsx', 'r')
zip_ref.extractall('/content2')
zip_ref.close()

pip install xgboost

pip install ydata-profiling

pip install imblearn

pip install openpyxl

import pandas as pd
import tensorflow as tf
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import ydata_profiling as dp
import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score , classification_report, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense
from sklearn.linear_model import LogisticRegression , LinearRegression, Ridge, Lasso
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler , LabelEncoder, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectKBest, chi2
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix, classification_report
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV

"""# 1 . Initial models (Scaling done beforehand)(without Smote , RFE , SelectKBest )

"""

df=pd.read_excel('/content/drive/MyDrive/Content /NEW DATASET EKG.xlsx')

new_df=df.copy()
df.shape

df.info()



prof_rep=dp.ProfileReport(df)
prof_rep

num=[feat for feat in df.columns if df[feat].dtype!='O']
cat=[feat for feat in df.columns if feat not in num]

num_df=df[num]
num_df
cat_df=df[cat]

ss=StandardScaler()
#mm=MinMaxScaler()
num_df_sc=ss.fit_transform(num_df)
num_df_sc=pd.DataFrame(num_df_sc,columns=num)
num_df_sc

cat_df

cat_df.drop(['Lifestyle Factors'],axis=1,inplace=True)

cat_df

ohe=OneHotEncoder(drop='first')
cat_df_ohe=ohe.fit_transform(cat_df.drop(['Disease Label'],axis=1))
cat_df_ohe.toarray()
cat_df_ohe=pd.DataFrame(cat_df_ohe.toarray(),columns=ohe.get_feature_names_out())
cat_df_ohe

encoder=LabelEncoder()
cat_df_ohe['Encoded_Disease_label'] = encoder.fit_transform(cat_df['Disease Label'])
cat_df_ohe

new_df=pd.concat([num_df_sc,cat_df_ohe],axis=1)
new_df

x_train,X_test,y_train,y_test=train_test_split(new_df.drop(['Encoded_Disease_label'],axis=1),new_df['Encoded_Disease_label'],test_size=0.2,random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import numpy as np

# Define stratified cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define models and hyperparameters
randomcv_models = [
    ("Logistic Regression", LogisticRegression(max_iter=1000),
        {'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'penalty': ['l1', 'l2', 'elasticnet', 'none'], 'solver': ['lbfgs', 'liblinear', 'saga']}),

    ("Decision Tree", DecisionTreeClassifier(),
        {'max_depth': [None, 5, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20], 'min_samples_leaf': [1, 2, 5, 10], 'max_features': [None, 'sqrt', 'log2']}),

    ("Random Forest", RandomForestClassifier(),
        {'n_estimators': [100, 200, 500, 1000], 'max_depth': [None, 5, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20], 'min_samples_leaf': [1, 2, 5, 10], 'max_features': [None, 'sqrt', 'log2']}),

   """ ("Gradient Boost", GradientBoostingClassifier(),
        {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7, 10], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 5]}),

    ("AdaBoost", AdaBoostClassifier(),
        {'n_estimators': [50, 100, 200, 500], 'learning_rate': [0.01, 0.1, 1.0, 10.0]}),

    ("XGBoost", XGBClassifier(),
        {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7, 10, 15], 'n_estimators': [100, 200, 300, 500], 'colsample_bytree': [0.5, 0.7, 0.9, 1.0], 'subsample': [0.5, 0.7, 0.9, 1.0], 'gamma': [0, 0.1, 0.2, 0.3]})
        """
]

# Determine the smallest class size
unique, counts = np.unique(y_train, return_counts=True)
min_samples = min(counts)

# Adjust k_neighbors dynamically based on the smallest class size
if min_samples > 1:
    k_neighbors = min(5, min_samples - 1)
else:
    k_neighbors = 1  # If only one sample, set k_neighbors to 1

# Apply SMOTE only if possible
if min_samples > 1:
    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
    X_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)
else:
    X_train_resampled, y_train_resampled = x_train, y_train  # No resampling if not enough samples

# Scale the data for models that require it
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name, model, param_grid in randomcv_models:
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
    grid_search.fit(x_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} Accuracy: {accuracy}")

log=LogisticRegression()
log.fit(x_train,y_train)
log.coef_

y_pred_log=log.predict(X_test)
print(classification_report(y_test,y_pred_log))



